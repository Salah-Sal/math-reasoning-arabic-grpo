model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  max_seq_length: 384
  load_in_4bit: true
  fast_inference: false
  gpu_memory_utilization: 0.7

peft:
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  lora_rank: 16
  lora_alpha: 16
  use_gradient_checkpointing: "unsloth"
  random_state: 3407

saving:
  output_dir: "models/qwen_arabic_math"
  checkpoint_dir: "checkpoints"
  save_method: "merged_4bit"
  save_steps: 500
  max_checkpoints: 3 